{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Google Tunix Hack - Train a Model to Show Its Work\n",
    "\n",
    "## Training Gemma 3 1B with GRPO for Chain-of-Thought Reasoning\n",
    "\n",
    "**Author:** Emrullah Aydogan  \n",
    "**Competition:** [Google Tunix Hack](https://www.kaggle.com/competitions/google-tunix-hackathon)  \n",
    "**Objective:** Train Gemma to show step-by-step reasoning on math problems\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Approach Summary\n",
    "\n",
    "**Model:** Gemma 3 1B (32K context, efficient)  \n",
    "**Algorithm:** GRPO (Group Relative Policy Optimization)  \n",
    "**Dataset:** GSM8K (8,500 math problems)  \n",
    "**Reward Function:**\n",
    "- 50% Correctness - Is the answer right?\n",
    "- 30% Reasoning Quality - Are steps logical?\n",
    "- 20% Clarity - Is explanation clear?\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Kaggle Setup Notes\n",
    "\n",
    "**Required Settings:**\n",
    "- ‚úÖ Accelerator: TPU VM v2-8 (or GPU T4)\n",
    "- ‚úÖ Internet: ON (for downloading model)\n",
    "- ‚úÖ Persistence: ON (optional, for checkpoints)\n",
    "\n",
    "**This notebook is STANDALONE** - all code included, no external dependencies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1Ô∏è‚É£ Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Install dependencies\n",
    "print(\"üì¶ Installing packages...\")\n",
    "\n",
    "!pip install -q google-tunix[prod] datasets transformers sentencepiece\n",
    "!pip install -q jax[tpu] jaxlib flax optax chex\n",
    "!pip install -q wandb pyyaml tqdm matplotlib seaborn\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Check Tunix\n",
    "try:\n",
    "    import tunix\n",
    "    print(f\"‚úÖ Tunix version: {tunix.__version__}\")\n",
    "    TUNIX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Tunix not available - will show placeholder implementation\")\n",
    "    TUNIX_AVAILABLE = False\n",
    "\n",
    "# Check device\n",
    "print(f\"\\nüñ•Ô∏è JAX Backend: {jax.default_backend()}\")\n",
    "print(f\"   Devices: {jax.devices()}\")\n",
    "print(f\"   Device count: {jax.device_count()}\")\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2Ô∏è‚É£ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model & Training Configuration\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'google/gemma-3-1b',\n",
    "    'use_lora': True,\n",
    "    'lora_rank': 16,\n",
    "    'lora_alpha': 32,\n",
    "    'lora_dropout': 0.1,\n",
    "    \n",
    "    # Training\n",
    "    'algorithm': 'GRPO',  # Group Relative Policy Optimization\n",
    "    'num_epochs': 3,\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 1e-5,\n",
    "    'warmup_steps': 100,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    \n",
    "    # RL Parameters\n",
    "    'num_rollouts': 4,\n",
    "    'temperature': 0.7,\n",
    "    'top_p': 0.9,\n",
    "    'max_new_tokens': 512,\n",
    "    \n",
    "    # Reward Weights\n",
    "    'correctness_weight': 0.5,\n",
    "    'reasoning_weight': 0.3,\n",
    "    'clarity_weight': 0.2,\n",
    "    \n",
    "    # Data\n",
    "    'val_ratio': 0.1,\n",
    "    'max_train_samples': None,  # None = use all\n",
    "    'max_eval_samples': 500,\n",
    "    \n",
    "    # Logging\n",
    "    'use_wandb': False,  # Set True to enable W&B\n",
    "    'wandb_project': 'google-tunix-hack',\n",
    "    'experiment_name': 'gemma3-1b-grpo-gsm8k',\n",
    "    'log_steps': 50,\n",
    "    'eval_steps': 250,\n",
    "    'save_steps': 500,\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3Ô∏è‚É£ Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing utilities\n",
    "\n",
    "def extract_answer(answer_text: str) -> str:\n",
    "    \"\"\"Extract final numerical answer from GSM8K answer text.\"\"\"\n",
    "    match = re.search(r'####\\s*(-?\\d+(?:,\\d{3})*(?:\\.\\d+)?)', answer_text)\n",
    "    if match:\n",
    "        return match.group(1).replace(',', '')\n",
    "    \n",
    "    # Fallback: find last number\n",
    "    numbers = re.findall(r'-?\\d+(?:,\\d{3})*(?:\\.\\d+)?', answer_text)\n",
    "    return numbers[-1].replace(',', '') if numbers else \"\"\n",
    "\n",
    "\n",
    "def extract_reasoning(answer_text: str) -> str:\n",
    "    \"\"\"Extract reasoning steps from GSM8K answer.\"\"\"\n",
    "    return re.split(r'####', answer_text)[0].strip()\n",
    "\n",
    "\n",
    "def format_cot_example(question: str, reasoning: str, answer: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Format as chain-of-thought training example.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'input', 'target', 'question', 'answer'\n",
    "    \"\"\"\n",
    "    # Prompt\n",
    "    input_text = f\"\"\"Question: {question}\n",
    "\n",
    "Let's solve this step by step:\n",
    "\"\"\"\n",
    "    \n",
    "    # Target with formatted steps\n",
    "    reasoning_lines = [line.strip() for line in reasoning.split('\\n') if line.strip()]\n",
    "    formatted_steps = [f\"Step {i}: {line}\" for i, line in enumerate(reasoning_lines, 1)]\n",
    "    \n",
    "    target_text = \"\\n\".join(formatted_steps) + f\"\\n\\nAnswer: {answer}\"\n",
    "    \n",
    "    return {\n",
    "        'input': input_text,\n",
    "        'target': target_text,\n",
    "        'question': question,\n",
    "        'answer': answer\n",
    "    }\n",
    "\n",
    "\n",
    "def preprocess_gsm8k(dataset_split, max_samples: Optional[int] = None) -> List[Dict]:\n",
    "    \"\"\"Preprocess GSM8K dataset split.\"\"\"\n",
    "    processed = []\n",
    "    \n",
    "    for example in tqdm(dataset_split, desc=\"Preprocessing\"):\n",
    "        reasoning = extract_reasoning(example['answer'])\n",
    "        answer = extract_answer(example['answer'])\n",
    "        formatted = format_cot_example(example['question'], reasoning, answer)\n",
    "        processed.append(formatted)\n",
    "        \n",
    "        if max_samples and len(processed) >= max_samples:\n",
    "            break\n",
    "    \n",
    "    return processed\n",
    "\n",
    "\n",
    "print(\"‚úÖ Data processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4Ô∏è‚É£ Reward Function (CRITICAL!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function for RL training\n",
    "\n",
    "def extract_model_answer(response: str) -> str:\n",
    "    \"\"\"Extract final answer from model response.\"\"\"\n",
    "    match = re.search(r'Answer:\\s*([^\\n]+)', response, re.IGNORECASE)\n",
    "    if match:\n",
    "        answer = match.group(1).strip()\n",
    "        number_match = re.search(r'-?\\d+(?:,\\d{3})*(?:\\.\\d+)?', answer)\n",
    "        if number_match:\n",
    "            return number_match.group(0).replace(',', '')\n",
    "    \n",
    "    # Fallback\n",
    "    numbers = re.findall(r'-?\\d+(?:,\\d{3})*(?:\\.\\d+)?', response)\n",
    "    return numbers[-1].replace(',', '') if numbers else \"\"\n",
    "\n",
    "\n",
    "def extract_reasoning_steps(response: str) -> List[str]:\n",
    "    \"\"\"Extract reasoning steps from model response.\"\"\"\n",
    "    step_pattern = r'Step \\d+:(.+?)(?=Step \\d+:|Answer:|$)'\n",
    "    steps = re.findall(step_pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "    return [step.strip() for step in steps if step.strip()]\n",
    "\n",
    "\n",
    "def check_correctness(predicted: str, ground_truth: str, tolerance: float = 1e-4) -> bool:\n",
    "    \"\"\"Check if predicted answer is correct.\"\"\"\n",
    "    try:\n",
    "        pred_num = float(predicted.replace(',', ''))\n",
    "        truth_num = float(ground_truth.replace(',', ''))\n",
    "        return abs(pred_num - truth_num) < tolerance\n",
    "    except:\n",
    "        return predicted.strip().lower() == ground_truth.strip().lower()\n",
    "\n",
    "\n",
    "def score_reasoning_quality(steps: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Score reasoning quality (0-1).\n",
    "    \n",
    "    Criteria:\n",
    "    - Number of steps (ideal: 2-8)\n",
    "    - Step length (ideal: 20-150 chars)\n",
    "    - Contains calculations\n",
    "    - Step completeness\n",
    "    \"\"\"\n",
    "    if not steps:\n",
    "        return 0.0\n",
    "    \n",
    "    score = 0.0\n",
    "    num_steps = len(steps)\n",
    "    \n",
    "    # 1. Number of steps (25%)\n",
    "    if 2 <= num_steps <= 8:\n",
    "        score += 0.25\n",
    "    elif num_steps == 1:\n",
    "        score += 0.1\n",
    "    elif num_steps > 8:\n",
    "        score += max(0.25 - 0.02 * (num_steps - 8), 0.1)\n",
    "    \n",
    "    # 2. Step length (25%)\n",
    "    avg_length = sum(len(s) for s in steps) / num_steps\n",
    "    if 20 <= avg_length <= 150:\n",
    "        score += 0.25\n",
    "    elif avg_length < 20:\n",
    "        score += 0.1\n",
    "    else:\n",
    "        score += max(0.25 - 0.001 * (avg_length - 150), 0.1)\n",
    "    \n",
    "    # 3. Contains calculations (25%)\n",
    "    calc_count = sum(1 for s in steps if re.search(r'\\d+\\s*[+\\-*/√ó√∑]\\s*\\d+', s))\n",
    "    score += min(0.25, calc_count * 0.1)\n",
    "    \n",
    "    # 4. Step completeness (25%)\n",
    "    complete_steps = sum(1 for s in steps if len(s) > 15 and any(c.isdigit() for c in s))\n",
    "    score += min(0.25, (complete_steps / max(num_steps, 1)) * 0.25)\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "\n",
    "def score_clarity(response: str, steps: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Score clarity and formatting (0-1).\n",
    "    \n",
    "    Criteria:\n",
    "    - Has step markers\n",
    "    - Has answer marker\n",
    "    - Proper punctuation\n",
    "    - Not repetitive\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    \n",
    "    # Step markers (30%)\n",
    "    if re.search(r'Step \\d+:', response, re.IGNORECASE):\n",
    "        score += 0.3\n",
    "    \n",
    "    # Answer marker (30%)\n",
    "    if re.search(r'Answer:', response, re.IGNORECASE):\n",
    "        score += 0.3\n",
    "    \n",
    "    # Punctuation (20%)\n",
    "    if any(char in response for char in '.!?'):\n",
    "        score += 0.2\n",
    "    \n",
    "    # Not repetitive (20%)\n",
    "    if len(steps) > 0:\n",
    "        unique_ratio = len(set(steps)) / len(steps)\n",
    "        score += unique_ratio * 0.2\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "\n",
    "def compute_reward(\n",
    "    response: str,\n",
    "    ground_truth: str,\n",
    "    question: str = \"\",\n",
    "    correctness_weight: float = 0.5,\n",
    "    reasoning_weight: float = 0.3,\n",
    "    clarity_weight: float = 0.2\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute comprehensive reward for model response.\n",
    "    \n",
    "    This is the MAIN reward function used for RL training.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all reward components\n",
    "    \"\"\"\n",
    "    # Extract components\n",
    "    predicted_answer = extract_model_answer(response)\n",
    "    steps = extract_reasoning_steps(response)\n",
    "    \n",
    "    # Compute scores\n",
    "    is_correct = check_correctness(predicted_answer, ground_truth)\n",
    "    correctness_score = 1.0 if is_correct else 0.0\n",
    "    reasoning_score = score_reasoning_quality(steps)\n",
    "    clarity_score = score_clarity(response, steps)\n",
    "    \n",
    "    # Weighted total\n",
    "    total_reward = (\n",
    "        correctness_weight * correctness_score +\n",
    "        reasoning_weight * reasoning_score +\n",
    "        clarity_weight * clarity_score\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'total_reward': total_reward,\n",
    "        'correctness_score': correctness_score,\n",
    "        'reasoning_score': reasoning_score,\n",
    "        'clarity_score': clarity_score,\n",
    "        'is_correct': is_correct,\n",
    "        'num_steps': len(steps),\n",
    "        'predicted_answer': predicted_answer,\n",
    "        'ground_truth': ground_truth\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Reward function defined\")\n",
    "\n",
    "# Test reward function\n",
    "test_response = \"\"\"Step 1: Janet's ducks lay 16 eggs per day\n",
    "Step 2: She uses 3 + 4 = 7 eggs\n",
    "Step 3: Remaining: 16 - 7 = 9 eggs\n",
    "Step 4: Revenue: 9 √ó $2 = $18\n",
    "\n",
    "Answer: 18\"\"\"\n",
    "\n",
    "reward = compute_reward(test_response, \"18\")\n",
    "print(f\"\\nüß™ Test Reward: {reward['total_reward']:.3f}\")\n",
    "print(f\"   ‚îú‚îÄ Correctness: {reward['correctness_score']:.3f}\")\n",
    "print(f\"   ‚îú‚îÄ Reasoning: {reward['reasoning_score']:.3f}\")\n",
    "print(f\"   ‚îî‚îÄ Clarity: {reward['clarity_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5Ô∏è‚É£ Load & Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load GSM8K from HuggingFace\n",
    "print(\"üì• Loading GSM8K dataset...\")\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded:\")\n",
    "print(f\"   Train: {len(dataset['train'])} samples\")\n",
    "print(f\"   Test: {len(dataset['test'])} samples\")\n",
    "\n",
    "# Show example\n",
    "example = dataset['train'][0]\n",
    "print(f\"\\nüìù Raw Example:\")\n",
    "print(f\"Q: {example['question'][:100]}...\")\n",
    "print(f\"A: {example['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Preprocess data\n",
    "print(\"üîÑ Preprocessing dataset...\\n\")\n",
    "\n",
    "train_data = preprocess_gsm8k(\n",
    "    dataset['train'],\n",
    "    max_samples=CONFIG['max_train_samples']\n",
    ")\n",
    "\n",
    "test_data = preprocess_gsm8k(\n",
    "    dataset['test'],\n",
    "    max_samples=CONFIG['max_eval_samples']\n",
    ")\n",
    "\n",
    "# Create validation split\n",
    "random.shuffle(train_data)\n",
    "val_size = int(len(train_data) * CONFIG['val_ratio'])\n",
    "val_data = train_data[:val_size]\n",
    "train_data = train_data[val_size:]\n",
    "\n",
    "print(f\"\\n‚úÖ Data prepared:\")\n",
    "print(f\"   Train: {len(train_data)} examples\")\n",
    "print(f\"   Validation: {len(val_data)} examples\")\n",
    "print(f\"   Test: {len(test_data)} examples\")\n",
    "\n",
    "# Show formatted example\n",
    "print(f\"\\nüìù Formatted Example:\")\n",
    "print(\"=\"*70)\n",
    "print(\"[INPUT]\")\n",
    "print(train_data[0]['input'])\n",
    "print(\"\\n[TARGET]\")\n",
    "print(train_data[0]['target'][:200] + \"...\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6Ô∏è‚É£ Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load tokenizer\n",
    "print(f\"üì• Loading tokenizer: {CONFIG['model_name']}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG['model_name'],\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded\")\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")\n",
    "print(f\"   Max length: {tokenizer.model_max_length}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = train_data[0]['input']\n",
    "tokens = tokenizer(test_text, return_tensors='np')\n",
    "print(f\"\\nüß™ Test tokenization:\")\n",
    "print(f\"   Input: {len(test_text)} chars\")\n",
    "print(f\"   Tokens: {len(tokens['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7Ô∏è‚É£ Training with Tunix GRPO\n",
    "\n",
    "### ‚ö†Ô∏è IMPORTANT: Actual Tunix Implementation Needed\n",
    "\n",
    "The cells below show the **intended structure** for Tunix training.  \n",
    "You need to implement actual Tunix GRPO trainer based on [Tunix documentation](https://github.com/google/tunix).\n",
    "\n",
    "**Tunix Example Notebooks:**\n",
    "- [GRPO on GSM8K](https://github.com/google/tunix/blob/main/examples/)\n",
    "- [QLoRA Fine-tuning](https://github.com/google/tunix/blob/main/examples/)\n",
    "\n",
    "**Key Integration Points:**\n",
    "1. Use our `compute_reward()` function as Tunix reward function\n",
    "2. Use our preprocessed `train_data` and `val_data`\n",
    "3. Configure LoRA with our CONFIG settings\n",
    "4. Log metrics to W&B (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B (optional)\n",
    "if CONFIG['use_wandb']:\n",
    "    try:\n",
    "        import wandb\n",
    "        wandb.init(\n",
    "            project=CONFIG['wandb_project'],\n",
    "            name=CONFIG['experiment_name'],\n",
    "            config=CONFIG\n",
    "        )\n",
    "        print(\"‚úÖ W&B initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è W&B initialization failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è W&B disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "print(\"üîß Training Configuration:\")\n",
    "print(f\"   Model: {CONFIG['model_name']}\")\n",
    "print(f\"   Algorithm: {CONFIG['algorithm']}\")\n",
    "print(f\"   Training samples: {len(train_data)}\")\n",
    "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"   Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"   Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"   LoRA: rank={CONFIG['lora_rank']}, alpha={CONFIG['lora_alpha']}\")\n",
    "print(f\"\\nüìä Reward Weights:\")\n",
    "print(f\"   Correctness: {CONFIG['correctness_weight']} (50%)\")\n",
    "print(f\"   Reasoning: {CONFIG['reasoning_weight']} (30%)\")\n",
    "print(f\"   Clarity: {CONFIG['clarity_weight']} (20%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNIX TRAINING IMPLEMENTATION GOES HERE\n",
    "# \n",
    "# Example structure (to be replaced with actual Tunix code):\n",
    "#\n",
    "# import tunix\n",
    "#\n",
    "# # Load model\n",
    "# model = tunix.load_model(CONFIG['model_name'])\n",
    "#\n",
    "# # Configure LoRA\n",
    "# lora_config = tunix.LoRAConfig(\n",
    "#     rank=CONFIG['lora_rank'],\n",
    "#     alpha=CONFIG['lora_alpha'],\n",
    "#     dropout=CONFIG['lora_dropout']\n",
    "# )\n",
    "#\n",
    "# # Create GRPO trainer\n",
    "# trainer = tunix.GRPOTrainer(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     train_dataset=train_data,\n",
    "#     eval_dataset=val_data,\n",
    "#     reward_function=lambda response, gt: compute_reward(response, gt, **CONFIG)['total_reward'],\n",
    "#     lora_config=lora_config,\n",
    "#     learning_rate=CONFIG['learning_rate'],\n",
    "#     num_epochs=CONFIG['num_epochs'],\n",
    "# )\n",
    "#\n",
    "# # Train\n",
    "# trainer.train()\n",
    "\n",
    "if TUNIX_AVAILABLE:\n",
    "    print(\"üöÄ Ready for Tunix training!\")\n",
    "    print(\"‚ö†Ô∏è Implement actual Tunix GRPO trainer above\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Tunix not available\")\n",
    "    print(\"‚ÑπÔ∏è This notebook shows the structure for Tunix integration\")\n",
    "    print(\"üìñ See: https://github.com/google/tunix for implementation details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8Ô∏è‚É£ Evaluation & Results\n",
    "\n",
    "After training, evaluate the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, tokenizer, test_data, max_samples=100):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set.\n",
    "    \n",
    "    Returns metrics and predictions.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    rewards = []\n",
    "    \n",
    "    print(f\"üìä Evaluating on {min(len(test_data), max_samples)} samples...\\n\")\n",
    "    \n",
    "    for i, example in enumerate(tqdm(test_data[:max_samples])):\n",
    "        # Generate response (placeholder - replace with actual model inference)\n",
    "        # response = model.generate(example['input'])\n",
    "        response = \"PLACEHOLDER - implement model inference\"\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = compute_reward(\n",
    "            response,\n",
    "            example['answer'],\n",
    "            example['question'],\n",
    "            CONFIG['correctness_weight'],\n",
    "            CONFIG['reasoning_weight'],\n",
    "            CONFIG['clarity_weight']\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'question': example['question'],\n",
    "            'response': response,\n",
    "            'ground_truth': example['answer'],\n",
    "            **reward\n",
    "        })\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    metrics = {\n",
    "        'accuracy': np.mean([r['is_correct'] for r in rewards]),\n",
    "        'avg_reward': np.mean([r['total_reward'] for r in rewards]),\n",
    "        'avg_reasoning_score': np.mean([r['reasoning_score'] for r in rewards]),\n",
    "        'avg_clarity_score': np.mean([r['clarity_score'] for r in rewards]),\n",
    "        'avg_num_steps': np.mean([r['num_steps'] for r in rewards]),\n",
    "    }\n",
    "    \n",
    "    return metrics, results\n",
    "\n",
    "\n",
    "print(\"‚úÖ Evaluation function defined\")\n",
    "print(\"‚ö†Ô∏è Requires trained model to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9Ô∏è‚É£ Visualization\n",
    "\n",
    "Visualize training and evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def plot_training_curves(history):\n",
    "    \"\"\"Plot training curves.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['loss'], label='Train Loss')\n",
    "    axes[0, 0].set_title('Training Loss')\n",
    "    axes[0, 0].set_xlabel('Step')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Reward\n",
    "    axes[0, 1].plot(history['reward'], label='Average Reward')\n",
    "    axes[0, 1].set_title('Training Reward')\n",
    "    axes[0, 1].set_xlabel('Step')\n",
    "    axes[0, 1].set_ylabel('Reward')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1, 0].plot(history['accuracy'], label='Accuracy')\n",
    "    axes[1, 0].set_title('Validation Accuracy')\n",
    "    axes[1, 0].set_xlabel('Step')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Reasoning score\n",
    "    axes[1, 1].plot(history['reasoning_score'], label='Reasoning Quality')\n",
    "    axes[1, 1].set_title('Reasoning Quality Score')\n",
    "    axes[1, 1].set_xlabel('Step')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_evaluation_results(metrics):\n",
    "    \"\"\"Plot evaluation metrics.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Bar chart of metrics\n",
    "    metric_names = ['Accuracy', 'Avg Reward', 'Reasoning', 'Clarity']\n",
    "    metric_values = [\n",
    "        metrics['accuracy'] * 100,\n",
    "        metrics['avg_reward'] * 100,\n",
    "        metrics['avg_reasoning_score'] * 100,\n",
    "        metrics['avg_clarity_score'] * 100\n",
    "    ]\n",
    "    \n",
    "    axes[0].bar(metric_names, metric_values, color=['#2ecc71', '#3498db', '#f39c12', '#e74c3c'])\n",
    "    axes[0].set_title('Evaluation Metrics (%)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Score (%)')\n",
    "    axes[0].set_ylim(0, 100)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate(metric_values):\n",
    "        axes[0].text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "    \n",
    "    # Pie chart of reward components\n",
    "    reward_components = [\n",
    "        metrics['avg_reasoning_score'] * CONFIG['reasoning_weight'],\n",
    "        metrics['accuracy'] * CONFIG['correctness_weight'],\n",
    "        metrics['avg_clarity_score'] * CONFIG['clarity_weight']\n",
    "    ]\n",
    "    \n",
    "    axes[1].pie(\n",
    "        reward_components,\n",
    "        labels=['Reasoning\\n(30%)', 'Correctness\\n(50%)', 'Clarity\\n(20%)'],\n",
    "        autopct='%1.1f%%',\n",
    "        colors=['#f39c12', '#2ecc71', '#e74c3c'],\n",
    "        startangle=90\n",
    "    )\n",
    "    axes[1].set_title('Reward Component Contribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîü Model Export\n",
    "\n",
    "Save the trained model for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model export\n",
    "OUTPUT_DIR = \"./trained_model\"\n",
    "\n",
    "def export_model(model, tokenizer, output_dir=OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Export trained model and tokenizer.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"üíæ Exporting model to {output_dir}...\")\n",
    "    \n",
    "    # Save model (implement based on Tunix API)\n",
    "    # model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # Save config\n",
    "    with open(f\"{output_dir}/training_config.json\", 'w') as f:\n",
    "        json.dump(CONFIG, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Model exported to {output_dir}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Export function defined\")\n",
    "print(f\"‚ÑπÔ∏è Model will be saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Summary & Next Steps\n",
    "\n",
    "### What We Built\n",
    "\n",
    "This notebook provides a **complete pipeline** for training Gemma with chain-of-thought reasoning:\n",
    "\n",
    "‚úÖ **Data Processing**\n",
    "- GSM8K dataset loading\n",
    "- Chain-of-thought formatting\n",
    "- Train/val/test splits\n",
    "\n",
    "‚úÖ **Reward Function** (CRITICAL!)\n",
    "- Multi-criteria evaluation\n",
    "- 50% correctness + 30% reasoning + 20% clarity\n",
    "- Comprehensive step analysis\n",
    "\n",
    "‚úÖ **Training Infrastructure**\n",
    "- Configuration management\n",
    "- LoRA settings\n",
    "- W&B logging support\n",
    "- Evaluation framework\n",
    "\n",
    "### What's Next\n",
    "\n",
    "1. **Implement Tunix GRPO Training** (Section 7)\n",
    "   - Use Tunix documentation\n",
    "   - Integrate our reward function\n",
    "   - Run on Kaggle TPU\n",
    "\n",
    "2. **Train & Evaluate**\n",
    "   - Start with small dataset (100 samples)\n",
    "   - Validate reward function works\n",
    "   - Scale to full dataset\n",
    "\n",
    "3. **Submission**\n",
    "   - Make notebook public\n",
    "   - Write Kaggle writeup\n",
    "   - Record YouTube video\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Resources\n",
    "\n",
    "- **Tunix GitHub:** https://github.com/google/tunix\n",
    "- **Competition:** https://www.kaggle.com/competitions/google-tunix-hackathon\n",
    "- **Project Repo:** https://github.com/EmrullahAydogan/Google_Tunix_Hack_Project\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "accelerator": "TPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
