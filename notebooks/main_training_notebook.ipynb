{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Tunix Hack - Train a Model to Show Its Work\n",
    "\n",
    "## Training Gemma 3 1B with GRPO for Chain-of-Thought Reasoning\n",
    "\n",
    "**Author:** Emrullah Aydogan  \n",
    "**Competition:** [Google Tunix Hack](https://www.kaggle.com/competitions/google-tunix-hackathon)  \n",
    "**Goal:** Train Gemma to show step-by-step reasoning on math problems\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Table of Contents\n",
    "\n",
    "1. [Setup & Installation](#1-setup)\n",
    "2. [Data Loading & Preprocessing](#2-data)\n",
    "3. [Model Configuration](#3-model)\n",
    "4. [Reward Function](#4-reward)\n",
    "5. [Training with Tunix GRPO](#5-training)\n",
    "6. [Evaluation](#6-evaluation)\n",
    "7. [Results & Visualization](#7-results)\n",
    "8. [Model Export](#8-export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Installation <a name=\"1-setup\"></a>\n",
    "\n",
    "Install required packages and setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "!pip install -q google-tunix[prod] datasets transformers sentencepiece\n",
    "!pip install -q jax jaxlib flax optax\n",
    "!pip install -q wandb rich pyyaml matplotlib seaborn\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Tunix imports (when available)\n",
    "try:\n",
    "    import tunix\n",
    "    print(f\"‚úÖ Tunix version: {tunix.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Tunix not found - will use placeholder implementation\")\n",
    "\n",
    "# Check TPU\n",
    "print(f\"\\nüñ•Ô∏è Available devices: {jax.devices()}\")\n",
    "print(f\"   Device count: {jax.device_count()}\")\n",
    "print(f\"   Platform: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading & Preprocessing <a name=\"2-data\"></a>\n",
    "\n",
    "Load GSM8K dataset and prepare for chain-of-thought training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GSM8K dataset\n",
    "print(\"üì• Loading GSM8K dataset...\")\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded:\")\n",
    "print(f\"   Train: {len(dataset['train'])} samples\")\n",
    "print(f\"   Test: {len(dataset['test'])} samples\")\n",
    "\n",
    "# Show example\n",
    "example = dataset['train'][0]\n",
    "print(f\"\\nüìù Example:\")\n",
    "print(f\"Question: {example['question'][:150]}...\")\n",
    "print(f\"Answer: {example['answer'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for data preprocessing\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_answer(answer_text: str) -> str:\n",
    "    \"\"\"Extract final numerical answer from GSM8K format\"\"\"\n",
    "    match = re.search(r'####\\s*(-?\\d+(?:,\\d{3})*(?:\\.\\d+)?)', answer_text)\n",
    "    if match:\n",
    "        return match.group(1).replace(',', '')\n",
    "    return \"\"\n",
    "\n",
    "def extract_reasoning(answer_text: str) -> str:\n",
    "    \"\"\"Extract reasoning steps\"\"\"\n",
    "    return re.split(r'####', answer_text)[0].strip()\n",
    "\n",
    "def format_cot_example(question: str, reasoning: str, answer: str) -> Dict[str, str]:\n",
    "    \"\"\"Format as chain-of-thought example\"\"\"\n",
    "    # Input prompt\n",
    "    input_text = f\"\"\"Question: {question}\n",
    "\n",
    "Let's solve this step by step:\n",
    "\"\"\"\n",
    "    \n",
    "    # Target with reasoning\n",
    "    # Split reasoning into steps\n",
    "    reasoning_steps = reasoning.split('\\n')\n",
    "    formatted_steps = []\n",
    "    for i, step in enumerate(reasoning_steps, 1):\n",
    "        if step.strip():\n",
    "            formatted_steps.append(f\"Step {i}: {step.strip()}\")\n",
    "    \n",
    "    target_text = \"\\n\".join(formatted_steps) + f\"\\n\\nAnswer: {answer}\"\n",
    "    \n",
    "    return {\n",
    "        'input': input_text,\n",
    "        'target': target_text,\n",
    "        'question': question,\n",
    "        'answer': answer\n",
    "    }\n",
    "\n",
    "# Test preprocessing\n",
    "test_example = dataset['train'][0]\n",
    "reasoning = extract_reasoning(test_example['answer'])\n",
    "answer = extract_answer(test_example['answer'])\n",
    "formatted = format_cot_example(test_example['question'], reasoning, answer)\n",
    "\n",
    "print(\"üìù Formatted Example:\")\n",
    "print(\"\\n[INPUT]\")\n",
    "print(formatted['input'])\n",
    "print(\"\\n[TARGET]\")\n",
    "print(formatted['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all data\n",
    "print(\"üîÑ Preprocessing dataset...\")\n",
    "\n",
    "def preprocess_dataset(dataset_split):\n",
    "    processed = []\n",
    "    for example in dataset_split:\n",
    "        reasoning = extract_reasoning(example['answer'])\n",
    "        answer = extract_answer(example['answer'])\n",
    "        formatted = format_cot_example(example['question'], reasoning, answer)\n",
    "        processed.append(formatted)\n",
    "    return processed\n",
    "\n",
    "# Process splits\n",
    "train_data = preprocess_dataset(dataset['train'])\n",
    "test_data = preprocess_dataset(dataset['test'])\n",
    "\n",
    "# Create validation split (10%)\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(train_data)\n",
    "val_size = int(len(train_data) * 0.1)\n",
    "val_data = train_data[:val_size]\n",
    "train_data = train_data[val_size:]\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocessed:\")\n",
    "print(f\"   Train: {len(train_data)} examples\")\n",
    "print(f\"   Validation: {len(val_data)} examples\")\n",
    "print(f\"   Test: {len(test_data)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Configuration <a name=\"3-model\"></a>\n",
    "\n",
    "Load Gemma 3 1B model and configure for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"google/gemma-3-1b\"\n",
    "ALGORITHM = \"GRPO\"  # Group Relative Policy Optimization\n",
    "\n",
    "config = {\n",
    "    'model': {\n",
    "        'name': MODEL_NAME,\n",
    "        'use_flash_attention': True,\n",
    "    },\n",
    "    'training': {\n",
    "        'algorithm': ALGORITHM,\n",
    "        'num_epochs': 3,\n",
    "        'batch_size': 8,\n",
    "        'learning_rate': 1e-5,\n",
    "        'warmup_steps': 100,\n",
    "        'use_lora': True,\n",
    "        'lora_rank': 16,\n",
    "        'lora_alpha': 32,\n",
    "    },\n",
    "    'rl': {\n",
    "        'num_rollouts': 4,\n",
    "        'temperature': 0.7,\n",
    "        'max_new_tokens': 512,\n",
    "    },\n",
    "    'reward': {\n",
    "        'correctness_weight': 0.5,\n",
    "        'reasoning_weight': 0.3,\n",
    "        'clarity_weight': 0.2,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(f\"üì• Loading tokenizer: {MODEL_NAME}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded\")\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")\n",
    "print(f\"   Max length: {tokenizer.model_max_length}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = train_data[0]['input'][:100]\n",
    "tokens = tokenizer(test_text, return_tensors='jax')\n",
    "print(f\"\\nüìù Test tokenization:\")\n",
    "print(f\"   Input length: {len(test_text)} chars\")\n",
    "print(f\"   Token count: {len(tokens['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Reward Function <a name=\"4-reward\"></a>\n",
    "\n",
    "Define reward function for evaluating reasoning quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function implementation\n",
    "\n",
    "def extract_model_answer(response: str) -> str:\n",
    "    \"\"\"Extract final answer from model response\"\"\"\n",
    "    match = re.search(r'Answer:\\s*([^\\n]+)', response, re.IGNORECASE)\n",
    "    if match:\n",
    "        answer = match.group(1).strip()\n",
    "        number_match = re.search(r'-?\\d+(?:,\\d{3})*(?:\\.\\d+)?', answer)\n",
    "        if number_match:\n",
    "            return number_match.group(0).replace(',', '')\n",
    "    numbers = re.findall(r'-?\\d+(?:,\\d{3})*(?:\\.\\d+)?', response)\n",
    "    if numbers:\n",
    "        return numbers[-1].replace(',', '')\n",
    "    return \"\"\n",
    "\n",
    "def extract_steps(response: str) -> List[str]:\n",
    "    \"\"\"Extract reasoning steps from response\"\"\"\n",
    "    step_pattern = r'Step \\d+:(.+?)(?=Step \\d+:|Answer:|$)'\n",
    "    steps = re.findall(step_pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "    return [step.strip() for step in steps if step.strip()]\n",
    "\n",
    "def check_correctness(predicted: str, ground_truth: str) -> bool:\n",
    "    \"\"\"Check if answer is correct\"\"\"\n",
    "    try:\n",
    "        pred_num = float(predicted.replace(',', ''))\n",
    "        truth_num = float(ground_truth.replace(',', ''))\n",
    "        return abs(pred_num - truth_num) < 1e-4\n",
    "    except:\n",
    "        return predicted.strip().lower() == ground_truth.strip().lower()\n",
    "\n",
    "def score_reasoning_quality(steps: List[str]) -> float:\n",
    "    \"\"\"Score quality of reasoning steps (0-1)\"\"\"\n",
    "    if not steps:\n",
    "        return 0.0\n",
    "    \n",
    "    score = 0.0\n",
    "    num_steps = len(steps)\n",
    "    \n",
    "    # Number of steps (ideal: 2-8)\n",
    "    if 2 <= num_steps <= 8:\n",
    "        score += 0.25\n",
    "    elif num_steps == 1:\n",
    "        score += 0.1\n",
    "    \n",
    "    # Average step length (ideal: 20-150 chars)\n",
    "    avg_length = sum(len(s) for s in steps) / num_steps\n",
    "    if 20 <= avg_length <= 150:\n",
    "        score += 0.25\n",
    "    \n",
    "    # Contains calculations\n",
    "    calc_count = sum(1 for s in steps if re.search(r'\\d+\\s*[+\\-*/√ó√∑]\\s*\\d+', s))\n",
    "    if calc_count > 0:\n",
    "        score += min(0.25, calc_count * 0.1)\n",
    "    \n",
    "    # Step completeness\n",
    "    complete_steps = sum(1 for s in steps if len(s) > 15 and any(c.isdigit() for c in s))\n",
    "    score += min(0.25, (complete_steps / max(num_steps, 1)) * 0.25)\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "def score_clarity(response: str, steps: List[str]) -> float:\n",
    "    \"\"\"Score clarity of response (0-1)\"\"\"\n",
    "    score = 0.0\n",
    "    \n",
    "    # Has step markers\n",
    "    if re.search(r'Step \\d+:', response, re.IGNORECASE):\n",
    "        score += 0.3\n",
    "    \n",
    "    # Has answer marker\n",
    "    if re.search(r'Answer:', response, re.IGNORECASE):\n",
    "        score += 0.3\n",
    "    \n",
    "    # Has punctuation\n",
    "    if any(char in response for char in '.!?'):\n",
    "        score += 0.2\n",
    "    \n",
    "    # Not repetitive\n",
    "    if len(steps) > 0:\n",
    "        unique_steps = len(set(steps))\n",
    "        score += min(0.2, (unique_steps / len(steps)) * 0.2)\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "def compute_reward(response: str, ground_truth: str, question: str = \"\") -> Dict:\n",
    "    \"\"\"Compute comprehensive reward for response\"\"\"\n",
    "    predicted_answer = extract_model_answer(response)\n",
    "    steps = extract_steps(response)\n",
    "    \n",
    "    # Component scores\n",
    "    is_correct = check_correctness(predicted_answer, ground_truth)\n",
    "    correctness_score = 1.0 if is_correct else 0.0\n",
    "    reasoning_score = score_reasoning_quality(steps)\n",
    "    clarity_score = score_clarity(response, steps)\n",
    "    \n",
    "    # Weighted total\n",
    "    total_reward = (\n",
    "        config['reward']['correctness_weight'] * correctness_score +\n",
    "        config['reward']['reasoning_weight'] * reasoning_score +\n",
    "        config['reward']['clarity_weight'] * clarity_score\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'total_reward': total_reward,\n",
    "        'correctness': correctness_score,\n",
    "        'reasoning': reasoning_score,\n",
    "        'clarity': clarity_score,\n",
    "        'is_correct': is_correct,\n",
    "        'num_steps': len(steps),\n",
    "        'predicted': predicted_answer,\n",
    "        'ground_truth': ground_truth\n",
    "    }\n",
    "\n",
    "# Test reward function\n",
    "test_response = \"\"\"Step 1: Janet's ducks lay 16 eggs per day\n",
    "Step 2: She uses 3 eggs for breakfast\n",
    "Step 3: She uses 4 eggs for muffins\n",
    "Step 4: Total used: 3 + 4 = 7 eggs\n",
    "Step 5: Remaining: 16 - 7 = 9 eggs\n",
    "Step 6: Revenue: 9 √ó $2 = $18\n",
    "\n",
    "Answer: 18\"\"\"\n",
    "\n",
    "reward = compute_reward(test_response, \"18\")\n",
    "print(\"\\nüéØ Reward Function Test:\")\n",
    "print(f\"  Total Reward: {reward['total_reward']:.3f}\")\n",
    "print(f\"  ‚îú‚îÄ Correctness: {reward['correctness']:.3f}\")\n",
    "print(f\"  ‚îú‚îÄ Reasoning: {reward['reasoning']:.3f}\")\n",
    "print(f\"  ‚îî‚îÄ Clarity: {reward['clarity']:.3f}\")\n",
    "print(f\"  Is Correct: {reward['is_correct']}\")\n",
    "print(f\"  Num Steps: {reward['num_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Training with Tunix GRPO <a name=\"5-training\"></a>\n",
    "\n",
    "**Note:** This section requires actual Tunix implementation.  \n",
    "The code below is a placeholder showing the intended structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "print(\"üîß Setting up training...\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Algorithm: {ALGORITHM}\")\n",
    "print(f\"   Training samples: {len(train_data)}\")\n",
    "print(f\"   Validation samples: {len(val_data)}\")\n",
    "\n",
    "# Initialize W&B (optional)\n",
    "try:\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        project=\"google-tunix-hack\",\n",
    "        name=\"gemma3-1b-grpo\",\n",
    "        config=config\n",
    "    )\n",
    "    print(\"‚úÖ W&B initialized\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è W&B not available\")\n",
    "\n",
    "# TODO: Actual Tunix training implementation\n",
    "# This will use the Tunix library's GRPO trainer\n",
    "print(\"\\n‚ö†Ô∏è Tunix training implementation goes here\")\n",
    "print(\"See Tunix documentation for GRPO trainer setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Evaluation <a name=\"6-evaluation\"></a>\n",
    "\n",
    "Evaluate trained model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation placeholder\n",
    "print(\"üìä Evaluating model...\")\n",
    "print(\"\\n‚ö†Ô∏è Evaluation implementation goes here\")\n",
    "print(\"Will evaluate on test set and compute:\")\n",
    "print(\"  - Accuracy\")\n",
    "print(\"  - Reasoning quality\")\n",
    "print(\"  - Clarity score\")\n",
    "print(\"  - Average number of steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Results & Visualization <a name=\"7-results\"></a>\n",
    "\n",
    "Visualize training results and model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results visualization placeholder\n",
    "print(\"üìà Visualization placeholder\")\n",
    "print(\"Will show:\")\n",
    "print(\"  - Training curves\")\n",
    "print(\"  - Reward progression\")\n",
    "print(\"  - Example predictions\")\n",
    "print(\"  - Accuracy by problem type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Model Export <a name=\"8-export\"></a>\n",
    "\n",
    "Save trained model for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model export\n",
    "output_dir = \"./trained_model\"\n",
    "print(f\"üíæ Exporting model to {output_dir}\")\n",
    "print(\"\\n‚ö†Ô∏è Model export implementation goes here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "This notebook demonstrates training Gemma 3 1B with Tunix GRPO for chain-of-thought reasoning.\n",
    "\n",
    "**Key Components:**\n",
    "- ‚úÖ Data preprocessing for GSM8K\n",
    "- ‚úÖ Multi-criteria reward function\n",
    "- ‚úÖ GRPO training configuration\n",
    "- ‚ö†Ô∏è Tunix training (to be implemented)\n",
    "- ‚ö†Ô∏è Evaluation framework (to be implemented)\n",
    "\n",
    "**Next Steps:**\n",
    "1. Implement actual Tunix GRPO training\n",
    "2. Run full training on TPU\n",
    "3. Evaluate on test set\n",
    "4. Create visualizations\n",
    "5. Export final model\n",
    "6. Write Kaggle writeup\n",
    "7. Record YouTube video\n",
    "\n",
    "---\n",
    "\n",
    "**Repository:** [GitHub](https://github.com/EmrullahAydogan/Google_Tunix_Hack_Project)  \n",
    "**Competition:** [Google Tunix Hack](https://www.kaggle.com/competitions/google-tunix-hackathon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
