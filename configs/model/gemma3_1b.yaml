# Gemma 3 1B Model Configuration
# Google Tunix Hack Competition

model_name: "gemma-3-1b"
model_id: "google/gemma-3-1b"

# Model Architecture
architecture:
  hidden_size: 2048
  num_attention_heads: 8
  num_hidden_layers: 18
  intermediate_size: 16384
  num_key_value_heads: 1  # Multi-query attention
  vocab_size: 256000
  max_position_embeddings: 32768  # 32K context window

# Attention Configuration
attention:
  attention_type: "local_global"
  local_attention_window: 1024
  local_to_global_ratio: 5  # 5:1 ratio
  use_sliding_window: true
  attention_dropout: 0.0

# Generation Configuration
generation:
  max_length: 2048
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  do_sample: true
  num_beams: 1  # Greedy for training, can increase for inference

# Tokenizer Configuration
tokenizer:
  tokenizer_name: "google/gemma-3-1b"
  use_fast: true
  padding_side: "left"
  truncation_side: "right"
  model_max_length: 32768

# Optimization
optimization:
  optimizer: "adamw"
  scheduler: "cosine"
  warmup_ratio: 0.05
  min_learning_rate: 1.0e-6

# Quantization (if needed)
quantization:
  load_in_8bit: false
  load_in_4bit: false
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"

# Special Tokens
special_tokens:
  bos_token: "<bos>"
  eos_token: "<eos>"
  unk_token: "<unk>"
  pad_token: "<pad>"

# Task-specific
task:
  task_type: "reasoning"
  show_reasoning: true
  reasoning_delimiter: "\n\nStep"
  answer_delimiter: "\n\nAnswer:"
